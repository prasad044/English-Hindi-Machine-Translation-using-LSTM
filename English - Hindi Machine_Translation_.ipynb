{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"About_Data\"></a>\n",
    "# <p style=\"padding:10px;background-color:lightblue;margin:0;color:#c27849;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Machine Translation</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#d5eae9; padding:20px; font-size:15px\">\n",
    "\n",
    "**This notebook includes the following:**\n",
    "\n",
    "- Preprocessing\n",
    "- Data cleaning\n",
    "- Exploratory data analysis (EDA)\n",
    "- Preparing the data to train a model\n",
    "- Training and making predictions using various classification models\n",
    "- Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"About_Data\"></a>\n",
    "# <p style=\"padding:10px;background-color:lightblue;margin:0;color:#c27849;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Dataset importing</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "091ye90OGOls"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KAYEKregGpGH"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/parallel-n/IITB.en-hi.en\",'r',encoding='utf-8') as f:\n",
    "    english_sentences = f.read().split('\\n')\n",
    "\n",
    "with open(\"/content/drive/MyDrive/parallel-n/IITB.en-hi.hi\",'r',encoding='utf-8') as f:\n",
    "    hindi_sentences = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ErL9zQW0HEHy"
   },
   "outputs": [],
   "source": [
    "#Some parameters\n",
    "vocab_size = 10000\n",
    "total_sentences = 50000\n",
    "maxlen = 16\n",
    "epochs = 70\n",
    "validation_split = 0.05\n",
    "max_sentence_length= maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"About_Data\"></a>\n",
    "# <p style=\"padding:10px;background-color:lightblue;margin:0;color:#c27849;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Data Preprocessing</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zVdhsL9HGvjG"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d','',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V4GHp3L9HBdr"
   },
   "outputs": [],
   "source": [
    "eng_sentence = [preprocess(en) for en in english_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CesjeNXjRImv"
   },
   "outputs": [],
   "source": [
    "hindi_sentence = [re.sub('[a-zA-Z]','',preprocess(hi))for hi in hindi_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2q9jCL4Rp-6",
    "outputId": "e93202cd-6000-4835-bee7-d1b58476bb00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1659084, 1659084)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(eng_sentence),len(hindi_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F31DSEr2RqBg"
   },
   "outputs": [],
   "source": [
    "#Remove duplicate sentences\n",
    "english_unique = set()\n",
    "eng_sentence_temp = []\n",
    "hindi_sentence_temp = []\n",
    "l = len(english_sentences)\n",
    "for i in range(l):\n",
    "    if english_sentences[i] not in english_unique:\n",
    "        english_unique.add(eng_sentence[i])\n",
    "        eng_sentence_temp.append(eng_sentence[i])\n",
    "        hindi_sentence_temp.append(hindi_sentence[i])\n",
    "\n",
    "eng_sentence = eng_sentence_temp\n",
    "hindi_sentence = hindi_sentence_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sBBYVr7tRwlC"
   },
   "outputs": [],
   "source": [
    "en_data = []\n",
    "hi_data = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for (en,hi) in zip(eng_sentence, hindi_sentence):\n",
    "    l = min(len(en.split()), len(hi.split()))\n",
    "    if l <= maxlen:\n",
    "        en_data.append(en)\n",
    "        hi_data.append(hi)\n",
    "    cnt += 1\n",
    "    if cnt == total_sentences:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pp8XK7kmRwno",
    "outputId": "6f1ae486-71fc-4de1-cc19-ba94143f96db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49249, 49249)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_data), len(hi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5GFUJUmMIc-q"
   },
   "outputs": [],
   "source": [
    "hi_data = ['<START> ' + hi + ' <END>' for hi in hi_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkTPY08yIxZ8",
    "outputId": "4eddf6f1-96a4-4980-a78c-4be04cd7115b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49249"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xyE_bRmIJPL5",
    "outputId": "5d2d3d05-1c2a-4da4-b7db-d1985384ceae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab Size:  3622\n",
      "Hindi Vocab Size:  3835\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize English texts\n",
    "en_tokenizer = Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
    "en_tokenizer.fit_on_texts(en_data)\n",
    "en_sequences = en_tokenizer.texts_to_sequences(en_data)\n",
    "\n",
    "# Tokenize Hindi texts\n",
    "hi_tokenizer = Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
    "hi_tokenizer.fit_on_texts(hi_data)\n",
    "hi_sequences = hi_tokenizer.texts_to_sequences(hi_data)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate vocabulary sizes\n",
    "english_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "hindi_vocab_size = len(hi_tokenizer.word_index) + 1\n",
    "\n",
    "# Print vocabulary sizes\n",
    "print(\"English Vocab Size: \", english_vocab_size)\n",
    "print(\"Hindi Vocab Size: \", hindi_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sdlLAZvVJemr"
   },
   "outputs": [],
   "source": [
    "#Prepare encoder data\n",
    "encoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(en_sequences, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFtRwmnNJkcJ",
    "outputId": "16e7755c-c8ff-46f1-9900-2a3900e54722"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1127,   79,  200, ...,    0,    0,    0],\n",
       "       [ 984,  590, 1450, ...,    0,    0,    0],\n",
       "       [   2,  153,   25, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 685,    0,    0, ...,    0,    0,    0],\n",
       "       [ 181,    0,    0, ...,    0,    0,    0],\n",
       "       [  28, 1190,  281, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"About_Data\"></a>\n",
    "# <p style=\"padding:10px;background-color:lightblue;margin:0;color:#c27849;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Split Dataset for training and testing</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLdzO1QjJmio",
    "outputId": "d84faf12-21c4-4778-f9f0-a1f1e8bdd8c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 16) (45000, 16) (45000, 16)\n"
     ]
    }
   ],
   "source": [
    "#Prepare decoder data\n",
    "decoder_inputs = []\n",
    "decoder_outputs = []\n",
    "\n",
    "for hi in hi_sequences:\n",
    "    decoder_inputs.append(hi[:-1])\n",
    "    decoder_outputs.append(hi[1:])\n",
    "\n",
    "decoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs, maxlen=maxlen, padding='post')\n",
    "decoder_outputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Training and Testing split\n",
    "# 95%, 5%\n",
    "split = int(0.9 * total_sentences)\n",
    "\n",
    "X_train = [encoder_inputs[:split], decoder_inputs[:split]]\n",
    "y_train = decoder_outputs[:split]\n",
    "\n",
    "# Test data to evaluate our NMT model using BLEU score\n",
    "X_test = en_data[:split]\n",
    "y_test = hi_data[:split]\n",
    "\n",
    "print(X_train[0].shape, X_train[1].shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uK7Wx56CJtGC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class LSTMModel(tf.keras.Model):\n",
    "    def __init__(self, encoder_vocab_size=None, decoder_vocab_size=None, embedding_size=128, num_rnn_units=32, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        # encoder\n",
    "        self.input_1 = tf.keras.layers.InputLayer(input_shape=(None,), name='input_1')\n",
    "        self.embedding_1 = tf.keras.layers.Embedding(encoder_vocab_size, embedding_size, mask_zero=True, name='embedding_1')\n",
    "        self.encoder_lstm = tf.keras.layers.LSTM(num_rnn_units, return_state=True, name='encoder_lstm')\n",
    "        # decoder\n",
    "        self.input_2 = tf.keras.layers.InputLayer(input_shape=(None,), name='input_2')\n",
    "        self.embedding_2 = tf.keras.layers.Embedding(decoder_vocab_size, embedding_size, mask_zero=True, name='embedding_2')\n",
    "        self.decoder_lstm = tf.keras.layers.LSTM(num_rnn_units, activation='relu', return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "        self.token_layer = tf.keras.layers.Dense(decoder_vocab_size, activation='softmax', name='token_layer')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_input = self.input_1(inputs[0])\n",
    "        decoder_input = self.input_2(inputs[1])\n",
    "        # encode the inputs\n",
    "        encoder_embed = self.embedding_1(encoder_input)\n",
    "        # run rnn on the encoded sequence\n",
    "        _, state_h, state_c = self.encoder_lstm(encoder_embed)\n",
    "        # decode the target\n",
    "        decoder_embed = self.embedding_2(decoder_input)\n",
    "        x, _, _ = self.decoder_lstm(decoder_embed, initial_state=[state_h, state_c])\n",
    "        return self.token_layer(x)\n",
    "\n",
    "    def predict_sequence(self, text, input_tokenizer, output_tokenizer, max_len):\n",
    "        if type(text) != list:\n",
    "            text = [text]\n",
    "        input_sequence = input_tokenizer.texts_to_sequences(text)\n",
    "        if type(input_sequence) == list:\n",
    "            input_sequence = np.array(input_sequence)\n",
    "        encoder_embed = self.embedding_1(input_sequence)\n",
    "        # run rnn on the encoded sequence\n",
    "        _, next_h, next_c = self.encoder_lstm(encoder_embed)\n",
    "        curr_token = [[output_tokenizer.word_index['<START>']]]\n",
    "\n",
    "        out_seq = \"\"\n",
    "        for _ in range(max_len):\n",
    "            decoder_embedding = self.embedding_2(np.array(curr_token))\n",
    "            x, next_h, next_c = self.decoder_lstm(decoder_embedding, initial_state=[next_h, next_c])\n",
    "            x = self.token_layer(x)\n",
    "            next_token = np.argmax(x[0, 0, :])\n",
    "            next_word = output_tokenizer.index_word[next_token]\n",
    "            if next_word == \"<END>\":\n",
    "                break\n",
    "            curr_token[0][0] = next_token\n",
    "            out_seq += \" \" + next_word\n",
    "        return out_seq.strip()\n",
    "\n",
    "# Example usage:\n",
    "# lstm_model = LSTMModel(encoder_vocab_size=5000, decoder_vocab_size=5000)\n",
    "# result = lstm_model.predict_sequence(\"example input text\", input_tokenizer, output_tokenizer, max_len=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUPjbUseJybR",
    "outputId": "fe59711f-7f53-4838-a49b-01ad352fd4be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer decoder_lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     multiple                  927232    \n",
      "                                                                 \n",
      " encoder_lstm (LSTM)         multiple                  82176     \n",
      "                                                                 \n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     multiple                  981760    \n",
      "                                                                 \n",
      " decoder_lstm (LSTM)         multiple                  82176     \n",
      "                                                                 \n",
      " token_layer (Dense)         multiple                  249275    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2322619 (8.86 MB)\n",
      "Trainable params: 2322619 (8.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "en_hi_model = LSTMModel(encoder_vocab_size=english_vocab_size, decoder_vocab_size= hindi_vocab_size ,embedding_size=256, num_rnn_units=64)\n",
    "en_hi_model(np.array([[[1]*max_sentence_length], [[1]*max_sentence_length]]))\n",
    "en_hi_model.summary()\n",
    "# model.load_weights(Weights_DIR+\"\\\\model.h5\")\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLxm9UexJ1ir",
    "outputId": "93d409f0-43a7-48dd-f0c3-f32eb12246c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1336/1336 [==============================] - 89s 62ms/step - loss: 4.9285 - accuracy: 0.2651 - val_loss: 5.4377 - val_accuracy: 0.2728\n",
      "Epoch 2/30\n",
      "1336/1336 [==============================] - 74s 56ms/step - loss: 3.5892 - accuracy: 0.3900 - val_loss: 5.3362 - val_accuracy: 0.2797\n",
      "Epoch 3/30\n",
      "1336/1336 [==============================] - 76s 57ms/step - loss: 2.6268 - accuracy: 0.5193 - val_loss: 5.5850 - val_accuracy: 0.2827\n",
      "Epoch 4/30\n",
      "1336/1336 [==============================] - 75s 56ms/step - loss: 1.9210 - accuracy: 0.6328 - val_loss: 5.9532 - val_accuracy: 0.2910\n",
      "Epoch 5/30\n",
      "1336/1336 [==============================] - 74s 55ms/step - loss: 1.4418 - accuracy: 0.7179 - val_loss: 6.0772 - val_accuracy: 0.3013\n",
      "Epoch 6/30\n",
      "1336/1336 [==============================] - 73s 54ms/step - loss: 1.1262 - accuracy: 0.7800 - val_loss: 6.1129 - val_accuracy: 0.2941\n",
      "Epoch 7/30\n",
      "1336/1336 [==============================] - 72s 54ms/step - loss: 0.9160 - accuracy: 0.8243 - val_loss: 6.4841 - val_accuracy: 0.2950\n",
      "Epoch 8/30\n",
      "1336/1336 [==============================] - 72s 54ms/step - loss: 0.7738 - accuracy: 0.8542 - val_loss: 6.5194 - val_accuracy: 0.2970\n",
      "Epoch 9/30\n",
      "1336/1336 [==============================] - 72s 54ms/step - loss: 0.6690 - accuracy: 0.8746 - val_loss: 6.4932 - val_accuracy: 0.2978\n",
      "Epoch 10/30\n",
      "1336/1336 [==============================] - 72s 54ms/step - loss: 0.5915 - accuracy: 0.8914 - val_loss: 6.7157 - val_accuracy: 0.2886\n",
      "Epoch 11/30\n",
      "1336/1336 [==============================] - 71s 53ms/step - loss: 0.5285 - accuracy: 0.9033 - val_loss: 6.8243 - val_accuracy: 0.2939\n",
      "Epoch 12/30\n",
      "1336/1336 [==============================] - 73s 54ms/step - loss: 0.4878 - accuracy: 0.9117 - val_loss: 6.8482 - val_accuracy: 0.2830\n",
      "Epoch 13/30\n",
      "1336/1336 [==============================] - 71s 53ms/step - loss: 0.4513 - accuracy: 0.9180 - val_loss: 6.8904 - val_accuracy: 0.2687\n",
      "Epoch 14/30\n",
      "1336/1336 [==============================] - 71s 53ms/step - loss: 0.4177 - accuracy: 0.9243 - val_loss: 6.9940 - val_accuracy: 0.2813\n",
      "Epoch 15/30\n",
      "1336/1336 [==============================] - 74s 55ms/step - loss: 0.3861 - accuracy: 0.9293 - val_loss: 7.1827 - val_accuracy: 0.2811\n",
      "Epoch 16/30\n",
      "1336/1336 [==============================] - 72s 54ms/step - loss: 0.3596 - accuracy: 0.9338 - val_loss: 7.2046 - val_accuracy: 0.2616\n",
      "Epoch 17/30\n",
      "1336/1336 [==============================] - 73s 55ms/step - loss: 0.3395 - accuracy: 0.9374 - val_loss: 7.1611 - val_accuracy: 0.2697\n",
      "Epoch 18/30\n",
      "1336/1336 [==============================] - 72s 54ms/step - loss: 0.3220 - accuracy: 0.9402 - val_loss: 7.1286 - val_accuracy: 0.2656\n",
      "Epoch 19/30\n",
      "1336/1336 [==============================] - 71s 53ms/step - loss: 0.3047 - accuracy: 0.9432 - val_loss: 7.3305 - val_accuracy: 0.2627\n",
      "Epoch 20/30\n",
      "1336/1336 [==============================] - 74s 55ms/step - loss: 0.2921 - accuracy: 0.9448 - val_loss: 7.3843 - val_accuracy: 0.2610\n",
      "Epoch 21/30\n",
      "1336/1336 [==============================] - 73s 54ms/step - loss: 0.2774 - accuracy: 0.9471 - val_loss: 7.5432 - val_accuracy: 0.2564\n",
      "Epoch 22/30\n",
      "1336/1336 [==============================] - 70s 53ms/step - loss: 0.2688 - accuracy: 0.9484 - val_loss: 7.4993 - val_accuracy: 0.2547\n",
      "Epoch 23/30\n",
      "1336/1336 [==============================] - 74s 55ms/step - loss: 0.2579 - accuracy: 0.9508 - val_loss: 7.5429 - val_accuracy: 0.2664\n",
      "Epoch 24/30\n",
      "1336/1336 [==============================] - 73s 55ms/step - loss: 0.2509 - accuracy: 0.9510 - val_loss: 7.8554 - val_accuracy: 0.2636\n",
      "Epoch 25/30\n",
      "1336/1336 [==============================] - 72s 54ms/step - loss: 0.2478 - accuracy: 0.9511 - val_loss: 7.7154 - val_accuracy: 0.2620\n",
      "Epoch 26/30\n",
      "1336/1336 [==============================] - 75s 56ms/step - loss: 0.2426 - accuracy: 0.9522 - val_loss: 7.7408 - val_accuracy: 0.2557\n",
      "Epoch 27/30\n",
      "1336/1336 [==============================] - 71s 53ms/step - loss: 0.2405 - accuracy: 0.9524 - val_loss: 7.6294 - val_accuracy: 0.2486\n",
      "Epoch 28/30\n",
      "1336/1336 [==============================] - 72s 54ms/step - loss: 0.2349 - accuracy: 0.9527 - val_loss: 7.7936 - val_accuracy: 0.2426\n",
      "Epoch 29/30\n",
      "1336/1336 [==============================] - 72s 54ms/step - loss: 0.2282 - accuracy: 0.9539 - val_loss: 7.7415 - val_accuracy: 0.2325\n",
      "Epoch 30/30\n",
      "1336/1336 [==============================] - 74s 55ms/step - loss: 0.2230 - accuracy: 0.9543 - val_loss: 8.0074 - val_accuracy: 0.2495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7c58b135db40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save model after each epoch\n",
    "save_model_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"/content/drive/MyDrive/parallel-n/model6\",\n",
    "    monitor='val_accuracy',\n",
    "    mode='max'\n",
    ")\n",
    "en_hi_model.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])\n",
    "en_hi_model.fit(X_train, y_train, epochs=30,batch_size= 32, validation_split=validation_split, callbacks=[save_model_callback, tf.keras.callbacks.TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "FzZzqVjSKC4r"
   },
   "outputs": [],
   "source": [
    "en_hi_model.save_weights(r\"/content/drive/MyDrive/parallel-n/model6/en_hi_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"About_Data\"></a>\n",
    "# <p style=\"padding:10px;background-color:lightblue;margin:0;color:#c27849;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Testing ans Analysis</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Q--J84_OTqr",
    "outputId": "024864df-3a96-449f-8cde-546a69781804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  give your application an accessibility workout\n",
      "Prediction:  अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें\n",
      "Dataset Reference:  अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें\n",
      "\n",
      "Input:  accerciser accessibility explorer\n",
      "Prediction:  एक्सेर्साइसर पहुंचनीयता अन्वेषक\n",
      "Dataset Reference:  एक्सेर्साइसर पहुंचनीयता अन्वेषक\n",
      "\n",
      "Input:  the default plugin layout for the bottom panel\n",
      "Prediction:  नीचे के इंच फ़ाइल\n",
      "Dataset Reference:  निचले पटल के लिए डिफोल्ट प्लगइन खाका\n",
      "\n",
      "Input:  the default plugin layout for the top panel\n",
      "Prediction:  ऊपरी पटल के लिए डिफोल्ट प्लगइन खाका\n",
      "Dataset Reference:  ऊपरी पटल के लिए डिफोल्ट प्लगइन खाका\n",
      "\n",
      "Input:  a list of plugins that are disabled by default\n",
      "Prediction:  उन प्लगइनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है\n",
      "Dataset Reference:  उन प्लगइनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है\n",
      "\n",
      "Input:  highlight duration\n",
      "Prediction:  हाइलाइट अवधिः\n",
      "Dataset Reference:  अवधि को हाइलाइट रकें\n",
      "\n",
      "Input:  the duration of the highlight box when selecting accessible nodes\n",
      "Prediction:  पहुंचनीय आसंधि नोड को चुनते समय हाइलाइट बक्से की अवधि\n",
      "Dataset Reference:  पहुंचनीय आसंधि नोड को चुनते समय हाइलाइट बक्से की अवधि\n",
      "\n",
      "Input:  highlight border color\n",
      "Prediction:  सीमांत बोर्डर के रंग\n",
      "Dataset Reference:  सीमांत बोर्डर के रंग को हाइलाइट करें\n",
      "\n",
      "Input:  the color and opacity of the highlight border\n",
      "Prediction:  हाइलाइट किए गए सीमांत का रंग और अपारदर्शिता।\n",
      "Dataset Reference:  हाइलाइट किए गए सीमांत का रंग और अपारदर्शिता।\n",
      "\n",
      "Input:  highlight fill color\n",
      "Prediction:  भराई के रंग को हाइलाइट करें\n",
      "Dataset Reference:  भराई के रंग को हाइलाइट करें\n",
      "\n",
      "Input:  the color and opacity of the highlight fill\n",
      "Prediction:  हाइलाइट किया गया भराई का रंग और पारदर्शिता।\n",
      "Dataset Reference:  हाइलाइट किया गया भराई का रंग और पारदर्शिता।\n",
      "\n",
      "Input:  api browser\n",
      "Prediction:  एपीआई विचरक\n",
      "Dataset Reference:  एपीआई विचरक\n",
      "\n",
      "Input:  browse the various methods of the current accessible\n",
      "Prediction:  इस समय जिसे प्राप्त किया गया हो उसकी विभिन्न विधियों मेथड में विचरण करें\n",
      "Dataset Reference:  इस समय जिसे प्राप्त किया गया हो उसकी विभिन्न विधियों मेथड में विचरण करें\n",
      "\n",
      "Input:  hide private attributes\n",
      "Prediction:  निजी गुणों को छिपाएं\n",
      "Dataset Reference:  निजी गुणों को छिपाएं\n",
      "\n",
      "Input:  method\n",
      "Prediction:  विधि\n",
      "Dataset Reference:  विधि\n",
      "\n",
      "Input:  property\n",
      "Prediction:  गुणधर्म\n",
      "Dataset Reference:  गुणधर्म\n",
      "\n",
      "Input:  value\n",
      "Prediction:  मान\n",
      "Dataset Reference:  मान\n",
      "\n",
      "Input:  ipython console\n",
      "Prediction:  आईपाइथन कन्सोल\n",
      "Dataset Reference:  आईपाइथन कन्सोल\n",
      "\n",
      "Input:  interactive console for manipulating currently selected accessible\n",
      "Prediction:  इस समय चुने गए एक्सेसेबेल से काम लेने के लिए अंतर्क्रियात्मक कन्सोल\n",
      "Dataset Reference:  इस समय चुने गए एक्सेसेबेल से काम लेने के लिए अंतर्क्रियात्मक कन्सोल\n",
      "\n",
      "Input:  event monitor\n",
      "Prediction:  घटना मानिटर\n",
      "Dataset Reference:  घटना मानिटर\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing and Analysis\n",
    "\n",
    "candidates = []\n",
    "references = []\n",
    "\n",
    "ctr = 20\n",
    "i = 0\n",
    "\n",
    "while ctr > 0:\n",
    "    l = len(X_test[i].split())\n",
    "    if l <= maxlen:  # Choose only sentences of length in range [5,15]\n",
    "        pred_sentence = en_hi_model.predict_sequence(X_test[i], en_tokenizer, hi_tokenizer, max_len=maxlen)\n",
    "        candidates.append(pred_sentence.split())\n",
    "\n",
    "        print(\"Input: \", X_test[i])\n",
    "        print(\"Prediction: \", pred_sentence)\n",
    "\n",
    "        # Google Translated Reference (if using Google Translate API)\n",
    "        # google_translated_sentence = translate_client.translate(X_test[i], target_language='hi')['translatedText']\n",
    "        # print(\"Google Translated Reference: \", google_translated_sentence)\n",
    "\n",
    "        print(\"Dataset Reference: \", ' '.join(y_test[i].split()[1:-1]))\n",
    "        print()\n",
    "        references.append([y_test[i].split()[1:-1]])\n",
    "\n",
    "        ctr -= 1\n",
    "    i += 1\n",
    "\n",
    "# Printing candidates and references for evaluation\n",
    "# print(\"References:\", references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "KfXtP9leOXHe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
